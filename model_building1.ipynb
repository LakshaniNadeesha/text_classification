{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69d8c27-7eb2-4db8-97bb-bb402302723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('messages.csv',encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38058b26-cb32-45af-a0f7-cb92feef86ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>student</th>\n",
       "      <th>created</th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>attachment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>student 1</td>\n",
       "      <td>12 January 2010, 10:49 PM</td>\n",
       "      <td>About ISDN</td>\n",
       "      <td>Can any one tell me about ISDN?</td>\n",
       "      <td>0</td>\n",
       "      <td>C-TE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>student 1</td>\n",
       "      <td>student 2</td>\n",
       "      <td>13 January 2010, 12:43 PM</td>\n",
       "      <td>About ISDN</td>\n",
       "      <td>(ISDN) = Integrted Services Digital Netork is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C-IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>student 2</td>\n",
       "      <td>student 1</td>\n",
       "      <td>13 January 2010, 03:28 PM</td>\n",
       "      <td>About ISDN</td>\n",
       "      <td>Thank you friend.....</td>\n",
       "      <td>0</td>\n",
       "      <td>C-RA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>student 1</td>\n",
       "      <td>student 3</td>\n",
       "      <td>14 January 2010, 06:18 PM</td>\n",
       "      <td>About ISDN</td>\n",
       "      <td>IDSN is basicly a digital dailup connection, y...</td>\n",
       "      <td>0</td>\n",
       "      <td>C-EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>student 1</td>\n",
       "      <td>student 4</td>\n",
       "      <td>13 January 2010, 02:55 PM</td>\n",
       "      <td>About ISDN</td>\n",
       "      <td>Integrated Services Digital Network (ISDN) is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C-EX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      parent    student                    created     subject  \\\n",
       "0       none  student 1  12 January 2010, 10:49 PM  About ISDN   \n",
       "1  student 1  student 2  13 January 2010, 12:43 PM  About ISDN   \n",
       "2  student 2  student 1  13 January 2010, 03:28 PM  About ISDN   \n",
       "3  student 1  student 3  14 January 2010, 06:18 PM  About ISDN   \n",
       "4  student 1  student 4  13 January 2010, 02:55 PM  About ISDN   \n",
       "\n",
       "                                             message  attachment label  \n",
       "0                    Can any one tell me about ISDN?           0  C-TE  \n",
       "1  (ISDN) = Integrted Services Digital Netork is ...           0  C-IN  \n",
       "2                              Thank you friend.....           0  C-RA  \n",
       "3  IDSN is basicly a digital dailup connection, y...           0  C-EX  \n",
       "4  Integrated Services Digital Network (ISDN) is ...           0  C-EX  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8f6baf-ed2c-4836-bd60-8a784b36ed71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parent        0\n",
       "student       0\n",
       "created       0\n",
       "subject       0\n",
       "message       0\n",
       "attachment    0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e428ca0-773b-44b7-9f24-5274ec3fbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and other non-alphabetic characters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0542e40c-e368-4ed0-a2eb-af2069ff5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Named Entities\n",
    "from nltk import ne_chunk\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6e42be-085a-4e78-8005-ff21bf224c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Named Entities\n",
    "def count_named_entities(text):\n",
    "    named_entities = extract_named_entities(text)\n",
    "    count = sum(1 for chunk in named_entities if hasattr(chunk, 'label'))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef94ff88-3afb-41da-96b6-a0e8bb402ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cohesion\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def compute_cohesion(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    words = [word for sentence in tokenized_sentences for word in sentence if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Compute word overlap between consecutive sentences\n",
    "    word_overlap_count = 0\n",
    "    for i in range(len(tokenized_sentences) - 1):\n",
    "        sentence1 = set(tokenized_sentences[i])\n",
    "        sentence2 = set(tokenized_sentences[i + 1])\n",
    "        word_overlap_count += len(sentence1.intersection(sentence2))\n",
    "\n",
    "    # Compute cohesion as the total word overlap normalized by the total number of words\n",
    "    total_words = len(words)\n",
    "    cohesion = word_overlap_count / total_words if total_words > 0 else 0\n",
    "\n",
    "    return cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465ca2dd-6e3d-40b6-acc2-deb7747f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Coherence\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def calculate_coherence(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Calculate bigrams\n",
    "    bi_grams = list(bigrams(words))\n",
    "\n",
    "    # Calculate frequency distribution of words and bigrams\n",
    "    freq_dist_words = FreqDist(words)\n",
    "    freq_dist_bigrams = FreqDist(bi_grams)\n",
    "\n",
    "    # Calculate Pointwise Mutual Information (PMI)\n",
    "    coherence = sum([freq_dist_bigrams[bigram] * freq_dist_words[bigram[0]] * freq_dist_words[bigram[1]] for bigram in bi_grams])\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d991b3eb-a595-4105-994d-d5e8415191ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count of a given text\n",
    "\n",
    "def word_count(text):\n",
    "    # Use split() to break the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Count the number of words\n",
    "    count = len(words)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8d9b7f-4d83-4033-8e9a-9421d39b9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Feature to Your Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'messages' is your DataFrame with columns 'message' and 'label'\n",
    "data['named_entities_count'] = data['message'].apply(count_named_entities)\n",
    "data['cohesion'] = data['message'].apply(compute_cohesion)\n",
    "data['coherence'] = data['message'].apply(calculate_coherence)\n",
    "data['wc'] = data['message'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2124a8e-3cb9-4f57-b378-a2eb80ab0c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\label predictor\\model\\venv\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\label predictor\\model\\venv\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\label predictor\\model\\venv\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\label predictor\\model\\venv\\lib\\site-packages (from gensim) (1.11.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\label predictor\\model\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7061ce4a-e898-409c-985e-abba1aa5f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32786885245901637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        C-EX       0.27      0.31      0.29        39\n",
      "        C-IN       0.43      0.49      0.46        51\n",
      "        C-RA       0.00      0.00      0.00        15\n",
      "        C-TE       0.20      0.18      0.19        17\n",
      "\n",
      "    accuracy                           0.33       122\n",
      "   macro avg       0.22      0.24      0.23       122\n",
      "weighted avg       0.29      0.33      0.31       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Prepare the text data for Word2Vec\n",
    "sentences = [word_tokenize(text) for text in data['message']]\n",
    "vector_size = 100  # You can adjust the vector size based on your dataset and requirements\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Function to convert text to average Word2Vec vectors\n",
    "def text_to_vector(text):\n",
    "    words = word_tokenize(text)\n",
    "    vec_sum = np.zeros(vector_size)\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        if word in word2vec_model.wv:\n",
    "            vec_sum += word2vec_model.wv[word]\n",
    "            word_count += 1\n",
    "    if word_count > 0:\n",
    "        return vec_sum / word_count\n",
    "    else:\n",
    "        return vec_sum\n",
    "\n",
    "# Convert text data to Word2Vec vectors\n",
    "data['message_vector'] = data['message'].apply(text_to_vector)\n",
    "\n",
    "# Select features (X) and target labels (Y)\n",
    "X = data['message_vector']\n",
    "Y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  # Using StratifiedKFold for classification\n",
    "# accuracy_scores = cross_val_score(rf_classifier, X['message_vector'].tolist(), Y, cv=cv, scoring='accuracy')\n",
    "rf_classifier.fit(X_train.tolist(), Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = rf_classifier.predict(X_test.tolist())\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ce6bc5-859b-4e0b-afa2-7df2d33aa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file using pickle\n",
    "model_filename = \"rf_model.pickle\"\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump((rf_classifier,word2vec_model), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821190c5-42e3-4500-bd61-e4780ffb2840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
